{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Create_dataset import create_dataset_2_2\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "#setting the size of the plot\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "def h_fun(z):\n",
    "    s = 1/(1+np.exp(-z))    \n",
    "    return s\n",
    "\n",
    "def param_initial(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    return w, b\n",
    "    \n",
    "lam0 = np.ones((2,1));\n",
    "\n",
    "###############################\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    lam0 = np.ones((2,1));\n",
    "    m = X.T.shape[1];\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = h_fun(np.dot(w.T, X)+b); # compute activation\n",
    "    cost = -1/m*sum(np.squeeze(lam0[0,0]*Y*np.log(A)+lam0[1,0]*(1-Y)*np.log(1-A)))   # compute cost\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = 1/m*(np.dot(X,(A-Y).T))\n",
    "    db = 1/m*sum(np.squeeze(A-Y))\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost\n",
    "\n",
    "###############################\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    costs = []\n",
    "    for i in range(num_iterations): \n",
    "        # Cost and gradient calculation\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        # update rule\n",
    "        w = w-learning_rate*dw\n",
    "        b = b-learning_rate*db\n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    params = {\"w\": w,\"b\": b}\n",
    "    grads = {\"dw\": dw,\"db\": db}\n",
    "    return params, grads, costs\n",
    "\n",
    "##############################\n",
    "\n",
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = h_fun(np.dot(w.T,X)+b)  \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[:,i] <= 0.5:\n",
    "            A[:,i]=0\n",
    "        else:\n",
    "            A[:,i]=1\n",
    "    Y_prediction=A\n",
    "    assert(Y_prediction.shape == (1, m))   \n",
    "    return Y_prediction\n",
    "    \n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    # initialize parameters with zeros\n",
    "    w, b = param_initial(X_train.shape[0])\n",
    "    # Gradient descent\n",
    "    #print(w.shape[0])\n",
    "    #print(b.shape[0])\n",
    "    #print(w.shape)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
