{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Create_dataset import create_dataset_2_2\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "#setting the size of the plot\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "################################################\n",
    "\n",
    "def create_dataset_2_2(flag,prior1,prior2,sample_size,m01,m02,c0_11,c0_12,c0_21,c0_22,m11,m12,c1_11,c1_12,c1_21,c1_22,seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    classes = 2         #total number of classes \n",
    "    num = sample_size      #Sample size\n",
    "    #initializing mean values and setting up mean values\n",
    "    mean = np.zeros((2,4)) \n",
    "    mean[:,0] = [m01,m02] \n",
    "    mean[:,1] = [m11,m12]\n",
    "    \n",
    "    cov = np.zeros((2,2,4))\n",
    "    cov[:,:,0] = np.array([[c0_11,c0_12],[c0_21,c0_22]]) \n",
    "    cov[:,:,1] = np.array([[c1_11,c1_12],[c1_21,c1_22]])\n",
    "    \n",
    "    # priors for two classes\n",
    "    class_priors = [prior1, prior2]\n",
    "    \n",
    "    #creating a class  labels. first we create uniform dirtribution to create labels and then use those \n",
    "    #label distribution to select repective guassian distribution to pick data.\n",
    "    class_labels = np.zeros((1,num))     #initialization\n",
    "    #uniform distribution creation\n",
    "    class_labels[0,:] = (np.random.uniform(0,1,num) >= class_priors[0]).astype(int)\n",
    "    \n",
    "    X = np.zeros((classes,num))  #initialization of main dataset\n",
    "    \n",
    "    # Data Generation Process: Using uniformly distributed Class labels to pick from respective guassian distribution\n",
    "    for index in range(num):\n",
    "        if(class_labels[0,index] == 0):\n",
    "                X[:,index] = np.random.multivariate_normal(mean[:,0],cov[:,:,0],1)\n",
    "        else:\n",
    "                X[:,index] = np.random.multivariate_normal(mean[:,1],cov[:,:,1],1) \n",
    "                \n",
    "    if flag == 1:   \n",
    "        # Code to Plot the actual distribution\n",
    "        x00 = [i for i in range(class_labels.shape[1]) if (class_labels[0,i] == 0)]\n",
    "        x01 = [i for i in range(class_labels.shape[1]) if (class_labels[0,i] == 0)]\n",
    "        x10 = [i for i in range(class_labels.shape[1]) if (class_labels[0,i] == 1)]\n",
    "        x11 = [i for i in range(class_labels.shape[1]) if (class_labels[0,i] == 1 )]\n",
    "        plt.plot(X[0,x00],X[1,x00],'.',color ='g')\n",
    "        plt.plot(X[0,x11],X[1,x11],'+',color = 'r')\n",
    "        plt.xlabel(\"Feature x1\")\n",
    "        plt.ylabel(\"Feature x2\")\n",
    "        plt.legend([\"class 0\",'class 1'])\n",
    "        plt.title(\"Actual Class distribution\")\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return X,class_labels,mean,cov #return the transpose of data array (row matrix)\n",
    "\n",
    "######################################################################################################\n",
    "######################################################################################################\n",
    "\n",
    "def min_P_error_classifier(sample_size,class_prior0,class_prior1,dataset,orig_label,gmean,gcov):\n",
    "    \n",
    "    #As it is min P(error) classifer, we will always take 0/1 loss\n",
    "    loss = np.array([[0,1], [1,0]])\n",
    "    size = sample_size\n",
    "    prior = [class_prior0,class_prior1]\n",
    "    \n",
    "    mean = np.zeros((2,4)) \n",
    "    mean[:,0] = gmean[:,0] \n",
    "    mean[:,1] = gmean[:,1]\n",
    "    \n",
    "    cov = np.zeros((2,2,4))\n",
    "    cov[:,:,0] = gcov[:,:,0]\n",
    "    cov[:,:,1] = gcov[:,:,1]\n",
    "    \n",
    "    # Gamma/ threshold\n",
    "    gamma = ((loss[1,0]-loss[0,0])/(loss[1,0] - loss[1,1])) * (prior[0]/prior[1])\n",
    "    orig_labels = orig_label\n",
    "\n",
    "    \n",
    "    new_labels = np.zeros((1,size))\n",
    "    # Calculation for discriminant score and decisions\n",
    "    cond_pdf_class0_log = np.log((multivariate_normal.pdf(dataset.T,mean=mean[:,0],cov = cov[:,:,0])))\n",
    "    cond_pdf_class1_log = np.log((multivariate_normal.pdf(dataset.T,mean=mean[:,1],cov = cov[:,:,1])))\n",
    "    \n",
    "    discriminant_score = cond_pdf_class1_log - cond_pdf_class0_log\n",
    "\n",
    "\n",
    "    new_labels[0,:] = (discriminant_score >= np.log(gamma)).astype(int)\n",
    "\n",
    "    # Code to plot the distribution after Classification\n",
    "    x00 = [i for i in range(new_labels.shape[1]) if (orig_labels[0,i] == 0 and new_labels[0,i] == 0)]\n",
    "    x01 = [i for i in range(new_labels.shape[1]) if (orig_labels[0,i] == 0 and new_labels[0,i] == 1)]\n",
    "    x10 = [i for i in range(new_labels.shape[1]) if (orig_labels[0,i] == 1 and new_labels[0,i] == 0)]\n",
    "    x11 = [i for i in range(new_labels.shape[1]) if (orig_labels[0,i] == 1 and new_labels[0,i] == 1)]\n",
    "    plt.plot(dataset[0,x00],dataset[1,x00],'.',color ='g')\n",
    "    plt.plot(dataset[0,x01],dataset[1,x01],'.',color = 'r')\n",
    "    plt.plot(dataset[0,x11],dataset[1,x11],'+',color ='g')\n",
    "    plt.plot(dataset[0,x10],dataset[1,x10],'+',color = 'r')\n",
    "    plt.legend([\"class 0 correctly classified\",'class 0 wrongly classified','class 1 correctly classified','class 1 wrongly classified'])\n",
    "    plt.xlabel(\"Feature x1\")\n",
    "    plt.ylabel(\"Feature x2\")\n",
    "    plt.title('Distribution after classification')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    c0 = np.argwhere(orig_labels[0,:]==0).shape[0]\n",
    "    c1 = np.argwhere(orig_labels[0,:]==1).shape[0]\n",
    "    #print(\"Class 0:\",c0)\n",
    "    #print(\"Class 1:\",c1)\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    tpr = 0\n",
    "    fpr = 0\n",
    "    min_TPR = 0\n",
    "    min_FPR = 0\n",
    "    TPR = []\n",
    "    FPR = []\n",
    "    new_labels1 = np.zeros((1,size))\n",
    "    d_labels1 = np.zeros((1,size))\n",
    "    r=map(lambda x: x/10.0,range(0,500))\n",
    "    print(r)\n",
    "    for i in r:\n",
    "        gamma1 = i\n",
    "        #print(gamma)\n",
    "        new_labels1[0,:] = (discriminant_score >= np.log(gamma1)).astype(int)\n",
    "        #d_labels1[0,:] = discriminant_score >= np.log(gamma)\n",
    "        for i in range(new_labels1.shape[1]): \n",
    "            #print(\"innerforloop\")\n",
    "            if (orig_labels[0,i] == 1 and new_labels1[0,i] == 1):\n",
    "               TP += 1\n",
    "            if (orig_labels[0,i] == 0 and new_labels1[0,i] == 1):\n",
    "               FP += 1\n",
    "            if (orig_labels[0,i] == 0 and new_labels1[0,i] == 0):\n",
    "               TN += 1\n",
    "            if (orig_labels[0,i] == 1 and new_labels1[0,i] == 0):\n",
    "               FN += 1\n",
    "        tpr = TP / (TP+FN)\n",
    "        fpr = FP / (FP+TN)\n",
    "        TPR.append(tpr)\n",
    "        FPR.append(fpr)\n",
    "        if gamma1 == 9.00000:\n",
    "            min_TPR = tpr\n",
    "            min_FPR = fpr\n",
    "        \n",
    "\n",
    "    plt.plot(FPR,TPR,'-',color = 'r')\n",
    "    plt.plot(min_FPR,min_TPR, 'g*')\n",
    "    plt.legend([\"ROC Curve\",'Min P Error'])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #h = .01  # step size in the mesh\n",
    "    # create a mesh to plot in\n",
    "    hg = np.linspace(np.floor(min(dataset[:,0])),np.ceil(max(dataset[:,0])),1000);\n",
    "    vg = np.linspace(np.floor(min(dataset[:,1])),np.ceil(max(dataset[:,1])),1000);\n",
    "    z = np.zeros((1000,1000))\n",
    "    xy = np.array(np.meshgrid(hg,vg))\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            p1 = multivariate_normal.pdf(np.array(xy[0][i][j],xy[1][i][j]),mean=mean[:,1],cov = cov[:,:,1])\n",
    "            p2 = multivariate_normal.pdf(np.array(xy[0][i][j],xy[1][i][j]),mean=mean[:,0],cov = cov[:,:,0])\n",
    "            z[i][j] = np.log(p1) - np.log(p2) - np.log(9)\n",
    "    \n",
    "    q00 = [i for i in range(class_labels.shape[1]) if (class_labels[0,i] == 0)]\n",
    "    q01 = [i for i in range(class_labels.shape[1]) if (class_labels[0,i] == 0)]\n",
    "    q10 = [i for i in range(class_labels.shape[1]) if (class_labels[0,i] == 1)]\n",
    "    q11 = [i for i in range(class_labels.shape[1]) if (class_labels[0,i] == 1 )]\n",
    "    #plt.plot(dataset[0,q00],dataset[1,q00],'.',color ='g')\n",
    "    #plt.plot(dataset[0,q11],dataset[1,q11],'+',color = 'r')\n",
    "    #plt.xlabel(\"Feature x1\")\n",
    "    #plt.ylabel(\"Feature x2\")\n",
    "    #plt.legend([\"class 0\",'class 1'])\n",
    "    #plt.title(\"Actual Class distribution\")\n",
    "    #plt.show()\n",
    "    #plt.contour(xy[0], xy[1], z)  \n",
    "    #, cmap=plt.cm.Paired\n",
    "    '''\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "def h_fun(z):\n",
    "    s = 1/(1+np.exp(-z))    \n",
    "    return s\n",
    "\n",
    "def param_initial(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    lam0 = np.ones((2,1));\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    return w, b,lam0\n",
    "\n",
    "\n",
    "###############################\n",
    "\n",
    "def propagate(w, b, X, Y, lam0):\n",
    "    m = X.T.shape[1];\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = h_fun(np.dot(w.T, X)+b); # compute activation\n",
    "    print(lam0.shape);\n",
    "    print(Y.shape);\n",
    "    print(m.shape);\n",
    "    print(A.shape);\n",
    "    cost = -1/m*sum(np.squeeze(lam0[0,0]*Y*np.log(A)+lam0[1,0]*(1-Y)*np.log(1-A)))   # compute cost\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = 1/m*(np.dot(X,(A-Y).T))\n",
    "    db = 1/m*sum(np.squeeze(A-Y))\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost\n",
    "\n",
    "###############################\n",
    "\n",
    "def optimize(w, b, X, Y, l, num_iterations, learning_rate, print_cost = False):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations): \n",
    "        # Cost and gradient calculation\n",
    "        grads, cost = propagate(w, b, X, Y, l)\n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        # update rule\n",
    "        w = w-learning_rate*dw\n",
    "        b = b-learning_rate*db\n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return params, grads, costs\n",
    "\n",
    "##############################\n",
    "\n",
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = sigmoid(np.dot(w.T,X)+b)  \n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[:,i] <= 0.5:\n",
    "            A[:,i]=0\n",
    "        else:\n",
    "            A[:,i]=1\n",
    "    Y_prediction=A\n",
    "    assert(Y_prediction.shape == (1, m))   \n",
    "    return Y_prediction\n",
    "    \n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    # initialize parameters with zeros\n",
    "    w, b, l = param_initial(X_train.shape[0])\n",
    "    # Gradient descent\n",
    "    #print(w.shape[0])\n",
    "    #print(b.shape[0])\n",
    "    #print(w.shape)\n",
    "    parameters, grads, costs = optimize(w, b, l, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
